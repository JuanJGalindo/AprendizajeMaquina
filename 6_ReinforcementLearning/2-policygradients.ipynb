{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Policy Gradients\n\n- Para entrenar esta red se requieren definir las probabilidades objetivo `y`. \n\n- Si una acción es buena, deberiamos incrementar su  probabilidad, si es mala reducirla.\n\n- Cómo saber si es buena o mala?, algunas acciones pueden tener efectos retardados, dado que cuando se ganan o pierden puntos no se sabe de forma clara que acciones contribuyeron  (the _credit assignment problem_).\n\n- _Policy Gradients_ juega múltiples episodios y luego toma acciones de los buenos episiodios como las más probables, mientras que acciones de malos episiodios, como poco probables.\n\n- **Jugamos primero y luego revisamos que funcionó**\n\n- Creamos una función para jugar.\n\n- Se asume que la acción correcta es la derecha (1).\n\n- Se calcula el costo y sus gradientes (se guardarán y luego se modifican dependiendo de que tan buena o mala resultó la acción).","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport sys\nimport sklearn\n\n\n!pip install gymnasium[classic_control]  #install gymnasium and virtual display\n!pip install pyvirtualdisplay\nimport gym\nimport pyvirtualdisplay\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# To plot pretty figures\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\n\n# To get smooth animations\nimport matplotlib.animation as animation\nmpl.rc('animation', html='jshtml')\n\n# Where to save the figures\nPROJECT_ROOT_DIR = \".\"\nCHAPTER_ID = \"rl\"\nIMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\nos.makedirs(IMAGES_PATH, exist_ok=True)\n\ndef save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n    print(\"Saving figure\", fig_id)\n    if tight_layout:\n        plt.tight_layout()\n    plt.savefig(path, format=fig_extension, dpi=resolution)\n    \n    \ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\ndisplay","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def play_one_step(env, obs, model, loss_fn):\n    with tf.GradientTape() as tape:\n        left_proba = model(obs[np.newaxis])\n        action = (tf.random.uniform([1, 1]) > left_proba)\n        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n    grads = tape.gradient(loss, model.trainable_variables)\n    obs, reward, done, trun, info = env.step(int(action[0, 0].numpy()))\n    return obs, reward, done, grads","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Si `left_proba` es alta, `action` tenderá  a `False`. \n- Si se obtiene `False` significa 0 en el casteo a flotante, `y_target` será 1 - 0 = 1. \n- Fijamos el objetivo a 1, pretendiendo que la probabilidad de ir a izquierda debería ser 100%.","metadata":{}},{"cell_type":"code","source":"Se crea una función que desde `play_one_step()` juega múltiples episodios, retornando los rewards y gradients:","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n    all_rewards = []\n    all_grads = []\n    for episode in range(n_episodes):\n        current_rewards = []\n        current_grads = []\n        obs,_ = env.reset()\n        for step in range(n_max_steps):\n            obs, reward, done, grads = play_one_step(env, obs, model, loss_fn)\n            current_rewards.append(reward)\n            current_grads.append(grads)\n            if done:\n                break\n        all_rewards.append(current_rewards)\n        all_grads.append(current_grads)\n    return all_rewards, all_grads","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- El algritmo Policy Gradients utiliza el modelo para jugar varias veces un episodio (e.g., 10 tiempos), y luego revisará las rewards y normaliza. \n\n- Se construye una función para descontar rewards y otra para normalizar rewards a lo largo de muchos episodios.","metadata":{}},{"cell_type":"code","source":"def discount_rewards(rewards, discount_rate):\n    discounted = np.array(rewards)\n    for step in range(len(rewards) - 2, -1, -1): #acumular descuentos de fin a inicio\n        discounted[step] += discounted[step + 1] * discount_rate\n    return discounted\n\ndef discount_and_normalize_rewards(all_rewards, discount_rate):\n    all_discounted_rewards = [discount_rewards(rewards, discount_rate)\n                              for rewards in all_rewards]\n    flat_rewards = np.concatenate(all_discounted_rewards)\n    reward_mean = flat_rewards.mean()\n    reward_std = flat_rewards.std()\n    return [(discounted_rewards - reward_mean) / reward_std #normalización zscore\n            for discounted_rewards in all_discounted_rewards]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Por  ejemplo**: si tenemos 3 acciones, y después de cada acción se obtiene los rewards: 10 en la primera, 0 en al  segunda, y -50 en la tercera. \n\n- Si se utiliza un factor de descuento de 80%, en la 3rd acción se obtendrá -50 (full credit for the last reward), pero en la segunda solo -40 (80% credit del último reward), y para la 1st action obtendrá el 80% de -40 (-32) más crédito completo del primer reward (+10), lo cual conlleva a un descuento de reward de -22:","metadata":{}},{"cell_type":"code","source":"print(discount_rewards([10, 0, -50], discount_rate=0.8))\nprint(discount_rewards([10, 20], discount_rate=0.8))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Para normalizar todos los descuentos de rewards sobre los episodios, se cálcula la media y desviación estandar de todos los discounted rewards y se normaliza por zscore:","metadata":{}},{"cell_type":"code","source":"discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se definen los parámetros de simulación:","metadata":{}},{"cell_type":"code","source":"n_iterations = 150\nn_episodes_per_update = 10\nn_max_steps = 400\ndiscount_rate = 0.95\noptimizer = keras.optimizers.Adam(lr=0.01)\nloss_fn = keras.losses.binary_crossentropy\n\nkeras.backend.clear_session()\nnp.random.seed(42)\ntf.random.set_seed(42)\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(20, activation=\"elu\", input_shape=[4]),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make('CartPole-v1',render_mode=\"rgb_array\")\nenv.reset(seed=42)\n\n\nfor iteration in range(n_iterations):\n    all_rewards, all_grads = play_multiple_episodes(\n        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n    total_rewards = sum(map(sum, all_rewards))  #map aplica fun sobre items en all rewards                   \n    print(\"\\rIteration: {}, mean rewards: {:.1f}\".format(          \n        iteration, total_rewards / n_episodes_per_update), end=\"\") \n    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n                                                       discount_rate)\n    all_mean_grads = []\n    for var_index in range(len(model.trainable_variables)):\n        mean_grads = tf.reduce_mean(\n            [final_reward * all_grads[episode_index][step][var_index]\n             for episode_index, final_rewards in enumerate(all_final_rewards)\n                 for step, final_reward in enumerate(final_rewards)], axis=0)\n        all_mean_grads.append(mean_grads)\n    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\n\nenv.close()","metadata":{},"execution_count":null,"outputs":[]}]}