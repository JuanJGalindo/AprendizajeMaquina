{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Reinforcement Learning / Tensorflow - TF_Agents \n\nEl aprendizaje por refuerzo (RL) es uno de los campos más antiguos del aprendizaje automático. Ha existido desde la década de 1950 y ha producido muchas aplicaciones interesantes a lo largo de los años.\n\n<br />\n<img src='https://es.mathworks.com/help///reinforcement-learning/ug/reinforcement_learning_diagram.png' width='300' />\n\n*\"El aprendizaje por refuerzo se diferencia del aprendizaje supervisado en que no requiere la presentación de pares de entrada/salida etiquetados y no requiere que se corrijan explícitamente acciones subóptimas. En cambio, la atención se centra en encontrar un equilibrio entre la exploración (de territorio desconocido) y la explotación (del conocimiento actual)..\"* [wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)","metadata":{}},{"cell_type":"code","source":"#librerias necesarias\n!sudo apt-get update\n!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n!pip install 'imageio==2.4.0'\n!pip install pyvirtualdisplay\n!pip install tf-agents[reverb]\n!pip install pyglet\n!pip install swig\n!pip install gym[atari,box2d,accept-rom-license]  #install gym and virtual display","metadata":{"id":"KEHR2Ui-lo8O","execution":{"iopub.status.busy":"2023-11-07T00:32:21.921311Z","iopub.execute_input":"2023-11-07T00:32:21.922080Z","iopub.status.idle":"2023-11-07T00:36:25.618010Z","shell.execute_reply.started":"2023-11-07T00:32:21.922028Z","shell.execute_reply":"2023-11-07T00:36:25.616585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport pyvirtualdisplay\nimport reverb\n\nimport tensorflow as tf\n\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import sequential\nfrom tf_agents.policies import py_tf_eager_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import reverb_replay_buffer\nfrom tf_agents.replay_buffers import reverb_utils\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\n\nfrom tf_agents.environments import  suite_gym\nfrom tf_agents.environments.atari_preprocessing import AtariPreprocessing\nfrom tf_agents.environments.atari_wrappers import FrameStack4\nimport gym\n\n# To get smooth animations\nimport matplotlib.animation as animation\nmatplotlib.rc('animation', html='jshtml')","metadata":{"id":"sMitx5qSgJk1","execution":{"iopub.status.busy":"2023-11-07T00:37:15.537505Z","iopub.execute_input":"2023-11-07T00:37:15.538268Z","iopub.status.idle":"2023-11-07T00:37:22.118291Z","shell.execute_reply.started":"2023-11-07T00:37:15.538231Z","shell.execute_reply":"2023-11-07T00:37:22.117288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Setup...\n**Aprendendiendo a jugar River Raid** <br />\n<img src='https://s2.glbimg.com/bQhuS5w10e3MAFxzNtnBm_jJNVA=/695x0/s.glbimg.com/po/tt2/f/original/2016/02/26/river-raid-atari-2600-8.jpg' width='300' />\n<br />\n*\"River Raid: Varias revistas lo votaron como el mejor juego del año. En 1983, InfoWorld lo llamó el “videojuego” más desafiante.[2] En 1984, la revista The Desert News lo llamó: “El juego de guerra más jugable y divertido”.[3] Ese mismo año, el juego recibió el premio al \"mejor juego de acción del año\"1984\"[4], y un certificado al mérito en la categoría.\"1984 Best Computer Action Game\"* [Wikipedia](https://pt.wikipedia.org/wiki/River_Raid)","metadata":{}},{"cell_type":"code","source":"#Carregando - River raid\nenv = suite_gym.load(environment_name=\"RiverraidNoFrameskip-v4\",max_episode_steps=27000, gym_env_wrappers=[AtariPreprocessing,FrameStack4])\nenv","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:38:43.710719Z","iopub.execute_input":"2023-11-07T00:38:43.711630Z","iopub.status.idle":"2023-11-07T00:38:43.938862Z","shell.execute_reply.started":"2023-11-07T00:38:43.711588Z","shell.execute_reply":"2023-11-07T00:38:43.937732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.gym","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:38:48.767031Z","iopub.execute_input":"2023-11-07T00:38:48.767434Z","iopub.status.idle":"2023-11-07T00:38:48.774744Z","shell.execute_reply.started":"2023-11-07T00:38:48.767400Z","shell.execute_reply":"2023-11-07T00:38:48.773657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gym.envs.registry","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.seed(42)\nenv.reset()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:38:52.712631Z","iopub.execute_input":"2023-11-07T00:38:52.713020Z","iopub.status.idle":"2023-11-07T00:38:52.954684Z","shell.execute_reply.started":"2023-11-07T00:38:52.712991Z","shell.execute_reply":"2023-11-07T00:38:52.953475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nimg = env.render(mode=\"rgb_array\")\nplt.figure(figsize=(4, 6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:39:31.566486Z","iopub.execute_input":"2023-11-07T00:39:31.566917Z","iopub.status.idle":"2023-11-07T00:39:31.702959Z","shell.execute_reply.started":"2023-11-07T00:39:31.566885Z","shell.execute_reply":"2023-11-07T00:39:31.701778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up a virtual display for rendering OpenAI gym environments.\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()","metadata":{"id":"J6HsdS5GbSjd","execution":{"iopub.status.busy":"2023-11-06T14:49:18.974057Z","iopub.execute_input":"2023-11-06T14:49:18.975221Z","iopub.status.idle":"2023-11-06T14:49:19.484576Z","shell.execute_reply.started":"2023-11-06T14:49:18.975180Z","shell.execute_reply":"2023-11-06T14:49:19.483783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.version.VERSION","metadata":{"id":"NspmzG4nP3b9","execution":{"iopub.status.busy":"2023-11-06T14:49:22.787167Z","iopub.execute_input":"2023-11-06T14:49:22.787898Z","iopub.status.idle":"2023-11-06T14:49:22.795122Z","shell.execute_reply.started":"2023-11-06T14:49:22.787865Z","shell.execute_reply":"2023-11-06T14:49:22.794208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment Specifications\nTF-Agents proporciona especificaciones para observaciones, acciones y pasos, incluidas sus respectivas formas.","metadata":{}},{"cell_type":"code","source":"print('Acciones disponibles:\\n{}\\r\\n'.format(env.gym.get_action_meanings()))\nprint('Observaciones:\\n{}'.format(env.observation_spec()))","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:46:59.871343Z","iopub.execute_input":"2023-11-07T00:46:59.872596Z","iopub.status.idle":"2023-11-07T00:46:59.878725Z","shell.execute_reply.started":"2023-11-07T00:46:59.872548Z","shell.execute_reply":"2023-11-07T00:46:59.877626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment Wrappers","metadata":{}},{"cell_type":"code","source":"#Aquí está la lista de wrappers disponibles:\nimport tf_agents.environments.wrappers\n\nfor name in dir(tf_agents.environments.wrappers):\n    obj = getattr(tf_agents.environments.wrappers, name)\n    if hasattr(obj, \"__base__\") and issubclass(obj, tf_agents.environments.wrappers.PyEnvironmentBaseWrapper):\n        print(\"{:27s} {}\".format(name, obj.__doc__.split(\"\\n\")[0]))","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:48:56.961132Z","iopub.execute_input":"2023-11-07T00:48:56.961672Z","iopub.status.idle":"2023-11-07T00:48:56.971733Z","shell.execute_reply.started":"2023-11-07T00:48:56.961630Z","shell.execute_reply":"2023-11-07T00:48:56.970209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Algunos ejemplos de acciones en el juego\nenv.reset()\ntime_step = env.step(np.array(1)) # FIRE\ntime_step = env.step(np.array(3)) # RIGHT\ntime_step = env.step(np.array(8)) # DOWNRIGHT\n\nobservation = time_step.observation.astype(np.float32)\n\n#Como existen 3 canales de colores, no podemos mostrar 4 frames.\nimage = observation[..., :3]\nimage = np.clip(image / 150, 0, 1)\nplt.imshow(image)\nplt.axis(\"off\")\nprint(observation.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:55:20.569928Z","iopub.execute_input":"2023-11-07T00:55:20.570321Z","iopub.status.idle":"2023-11-07T00:55:20.674805Z","shell.execute_reply.started":"2023-11-07T00:55:20.570291Z","shell.execute_reply":"2023-11-07T00:55:20.673055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para agrupar el entorno utilizamos TFPyEnviroment.","metadata":{}},{"cell_type":"code","source":"from tf_agents.environments.tf_py_environment import TFPyEnvironment\ntf_env = TFPyEnvironment(env)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T00:56:11.246208Z","iopub.execute_input":"2023-11-07T00:56:11.246642Z","iopub.status.idle":"2023-11-07T00:56:11.268641Z","shell.execute_reply.started":"2023-11-07T00:56:11.246609Z","shell.execute_reply":"2023-11-07T00:56:11.267103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DQN\n- TF-Agents proporciona algunos paquetes de red.\n\n- En este paquete, las imágenes se almacenan utilizando bytes del 0 al 255 para utilizar menos RAM.\n","metadata":{}},{"cell_type":"code","source":"from tf_agents.networks.q_network import QNetwork\n#convertir observacinoes a float float 32, normalizando.. (valores  0.0 a 1.0) \npreprocessing_layer = tf.keras.layers.Lambda( lambda obs: tf.cast(obs, np.float32) / 255.)\n\n#arquitectura:\n#conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\nconv_layer_params=[(32, (8, 8), 4) , (64, (4, 4), 2)]#, (64, (3, 3), 1), (1024, (7, 7), 1)]\n#layer dense com 512 por uma cama de sair de 4 unidades\nfc_layer_params=(1024,)\n\nq_network = QNetwork(tf_env.observation_spec(), tf_env.action_spec()\n                     ,preprocessing_layers=preprocessing_layer\n                     ,conv_layer_params=conv_layer_params\n                     ,fc_layer_params=fc_layer_params)\nq_network.summary","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:00:17.676183Z","iopub.execute_input":"2023-11-07T01:00:17.676905Z","iopub.status.idle":"2023-11-07T01:00:17.700959Z","shell.execute_reply.started":"2023-11-07T01:00:17.676854Z","shell.execute_reply":"2023-11-07T01:00:17.699830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DQN Agent\n[DQN paper ](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)  ","metadata":{}},{"cell_type":"code","source":"from tf_agents.agents.dqn.dqn_agent import DqnAgent\n\ntrain_step = tf.Variable(0)\nupdate_period = 4 \n#optimizer = keras.optimizers.Adam(lr=2.5e-4, rho=0.95, momentum=0.1,epsilon=0.00001, centered=True)\noptimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate=2.5e-4, decay=0.95, momentum=0.0,\n                                     epsilon=0.00001, centered=True)\n#optimizer = keras.optimizers.Adam(lr=0.00005,epsilon=0.00001)\nepsilon_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=0.8,\n    decay_steps=250000 // update_period, \n    end_learning_rate=0.01)\n\nagent = DqnAgent(tf_env.time_step_spec(),\n                 tf_env.action_spec(),\n                 q_network=q_network,\n                 optimizer=optimizer,\n                 target_update_period=2000, \n                 #La función de pérdida debe devolver un error por instancia, por lo que definimos reducción=\"none\" \n                 td_errors_loss_fn=tf.keras.losses.Huber(reduction=\"none\"),\n                 gamma=0.89, # discount factor\n                 train_step_counter=train_step,\n                 epsilon_greedy=lambda: epsilon_fn(train_step))\nagent.initialize()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:01:47.808404Z","iopub.execute_input":"2023-11-07T01:01:47.808809Z","iopub.status.idle":"2023-11-07T01:01:48.157868Z","shell.execute_reply.started":"2023-11-07T01:01:47.808776Z","shell.execute_reply":"2023-11-07T01:01:48.156758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Se utiliza la [Huber loss](https://en.wikipedia.org/wiki/Huber_loss) como balance entre mse y mae","metadata":{}},{"cell_type":"markdown","source":"# Replay Buffer and the Corresponding Observer\n\n- La biblioteca TF-Agents proporciona algunas implementaciones de búfer de reproducción en el paquete tf_agents.replay_buffers.\n\n**max_length:**  1000000","metadata":{}},{"cell_type":"code","source":"from tf_agents.replay_buffers import tf_uniform_replay_buffer\n\n#data_spec datos que se guardarán en el búfer.\n#batch_size es el número de trayectorias que se deben agregar a cada paso.\n#max_length es la longitud máxima de reproducción. (Documento DQN2015: Cuidado con el acaparador de RAM)\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( data_spec=agent.collect_data_spec,    batch_size=tf_env.batch_size, max_length=300000)#ojo para el entrenamiento\n\nreplay_buffer_observer = replay_buffer.add_batch","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:05:29.689306Z","iopub.execute_input":"2023-11-07T01:05:29.689820Z","iopub.status.idle":"2023-11-07T01:05:34.270679Z","shell.execute_reply.started":"2023-11-07T01:05:29.689784Z","shell.execute_reply":"2023-11-07T01:05:34.269710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Metrics\n\nUsando las diversas métricas del paquete  tf_agents.metrics.","metadata":{}},{"cell_type":"code","source":"from tf_agents.metrics import tf_metrics\nfrom tf_agents.eval.metric_utils import log_metrics\nimport logging\n\ntrain_metrics = [\n    tf_metrics.NumberOfEpisodes(),\n    tf_metrics.EnvironmentSteps(),\n    tf_metrics.AverageReturnMetric(),\n    tf_metrics.AverageEpisodeLengthMetric(),\n]\nlogging.getLogger().setLevel(logging.INFO)\nlog_metrics(train_metrics)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:06:17.645971Z","iopub.execute_input":"2023-11-07T01:06:17.646416Z","iopub.status.idle":"2023-11-07T01:06:17.845600Z","shell.execute_reply.started":"2023-11-07T01:06:17.646359Z","shell.execute_reply":"2023-11-07T01:06:17.844403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collect Driver\n\nUn collect driver es un objeto que explora un entorno mediante políticas, recoge experiencias de cada etapa y las transmite a los observadores.","metadata":{}},{"cell_type":"code","source":"from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n\ncollect_driver = DynamicStepDriver(\n    tf_env,\n    agent.collect_policy,\n    observers=[replay_buffer_observer] + train_metrics,\n    num_steps=update_period) \ncollect_driver","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:07:16.002335Z","iopub.execute_input":"2023-11-07T01:07:16.002848Z","iopub.status.idle":"2023-11-07T01:07:16.012240Z","shell.execute_reply.started":"2023-11-07T01:07:16.002811Z","shell.execute_reply":"2023-11-07T01:07:16.010996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tf_agents.policies.random_tf_policy import RandomTFPolicy\n\nclass ShowProgress:\n    def __init__(self, total):\n        self.counter = 0\n        self.total = total\n    def __call__(self, trajectory):\n        if not trajectory.is_boundary():\n            self.counter += 1\n        if self.counter % 100 == 0:\n            print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\n\ninitial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n                                        tf_env.action_spec())\ninit_driver = DynamicStepDriver(\n    tf_env,\n    initial_collect_policy,\n    observers=[replay_buffer.add_batch, ShowProgress(20000)],\n    num_steps=20000)\nfinal_time_step, final_policy_state = init_driver.run()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:07:44.094251Z","iopub.execute_input":"2023-11-07T01:07:44.095510Z","iopub.status.idle":"2023-11-07T01:10:30.310646Z","shell.execute_reply.started":"2023-11-07T01:07:44.095448Z","shell.execute_reply":"2023-11-07T01:10:30.309283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ejemplo de la trayectoria final de un episodio\ntrajectories, buffer_info = replay_buffer.get_next(sample_batch_size=2, num_steps=17)\ntrajectories, buffer_info, trajectories._fields","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:14:08.694033Z","iopub.execute_input":"2023-11-07T01:14:08.694708Z","iopub.status.idle":"2023-11-07T01:14:08.750349Z","shell.execute_reply.started":"2023-11-07T01:14:08.694666Z","shell.execute_reply":"2023-11-07T01:14:08.748774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tf_agents.trajectories.trajectory import to_transition\n\ntime_steps, action_steps, next_time_steps = to_transition(trajectories)\ntime_steps.observation.shape,trajectories.step_type.numpy()\n\n\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img / 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:14:30.328093Z","iopub.execute_input":"2023-11-07T01:14:30.328979Z","iopub.status.idle":"2023-11-07T01:14:30.675269Z","shell.execute_reply.started":"2023-11-07T01:14:30.328939Z","shell.execute_reply":"2023-11-07T01:14:30.674083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset\n\nConvertir los datos del búfer en un conjunto de datos para el entrenamiento","metadata":{}},{"cell_type":"code","source":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=64,\n    num_steps=17,\n    num_parallel_calls=3\n).prefetch(3)\niterator = iter(dataset)\ntrajectories, buffer_info = next(iterator)\nplt.figure(figsize=(10, 6.8))\nfor row in range(2):\n    for col in range(3):\n        plt.subplot(2, 3, row * 3 + col + 1)\n        obs = trajectories.observation[row, col].numpy().astype(np.float32)\n        img = obs[..., :3]\n        current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n        img[..., 0] += current_frame_delta\n        img[..., 2] += current_frame_delta\n        img = np.clip(img / 150, 0, 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\nplt.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0, wspace=0.02)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:16:11.031072Z","iopub.execute_input":"2023-11-07T01:16:11.031585Z","iopub.status.idle":"2023-11-07T01:16:12.578270Z","shell.execute_reply.started":"2023-11-07T01:16:11.031543Z","shell.execute_reply":"2023-11-07T01:16:12.576709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = replay_buffer.as_dataset(\n    sample_batch_size=64,\n    num_steps=2,\n    num_parallel_calls=3).prefetch(3)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:16:19.815683Z","iopub.execute_input":"2023-11-07T01:16:19.816757Z","iopub.status.idle":"2023-11-07T01:16:20.422575Z","shell.execute_reply.started":"2023-11-07T01:16:19.816715Z","shell.execute_reply":"2023-11-07T01:16:20.421283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"#convertir funciones\nfrom tf_agents.utils.common import function\n\ncollect_driver.run = function(collect_driver.run)\nagent.train = function(agent.train)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:16:23.216817Z","iopub.execute_input":"2023-11-07T01:16:23.217509Z","iopub.status.idle":"2023-11-07T01:16:23.223247Z","shell.execute_reply.started":"2023-11-07T01:16:23.217468Z","shell.execute_reply":"2023-11-07T01:16:23.222177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_agent(n_iterations):\n    time_step = None\n    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n    iterator = iter(dataset)\n    for iteration in range(n_iterations):\n        time_step, policy_state = collect_driver.run(time_step, policy_state)\n        trajectories, buffer_info = next(iterator)\n        train_loss = agent.train(trajectories)\n        print(\"\\r{} loss: {:.5f}\".format(\n            iteration, train_loss.loss.numpy()), end=\"\")\n        if iteration % 1000 == 0:\n            log_metrics(train_metrics)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:16:26.186541Z","iopub.execute_input":"2023-11-07T01:16:26.187616Z","iopub.status.idle":"2023-11-07T01:16:26.195270Z","shell.execute_reply.started":"2023-11-07T01:16:26.187570Z","shell.execute_reply":"2023-11-07T01:16:26.194206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#el valor ideal para n_iterations es 1.000.000\nnum_iterations_ = 900000 #ojo entrenamiento\ntrain_agent(n_iterations=num_iterations_)","metadata":{"execution":{"iopub.status.busy":"2023-11-07T01:18:38.096158Z","iopub.execute_input":"2023-11-07T01:18:38.096904Z","iopub.status.idle":"2023-11-07T01:18:38.426074Z","shell.execute_reply.started":"2023-11-07T01:18:38.096874Z","shell.execute_reply":"2023-11-07T01:18:38.424889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualization","metadata":{}},{"cell_type":"code","source":"def update_scene(num, frames, patch):\n    patch.set_data(frames[num])\n    return patch\n\ndef plot_animation(frames, repeat=False, interval=40):\n    fig = plt.figure()\n    patch = plt.imshow(frames[0])\n    plt.axis('off')\n    anim = animation.FuncAnimation(\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()\n    return anim","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frames = []\ndef save_frames(trajectory):\n    global frames\n    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n\nprev_lives = tf_env.pyenv.envs[0].ale.lives()\ndef reset_and_fire_on_life_lost(trajectory):\n    global prev_lives\n    lives = tf_env.pyenv.envs[0].ale.lives()\n    if prev_lives != lives:\n        tf_env.reset()\n        tf_env.pyenv.envs[0].step(np.array(tf_env.pyenv.envs[0].action_space.sample()))\n        prev_lives = lives\n\nwatch_driver = DynamicStepDriver(\n    tf_env,\n    agent.policy,\n    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(10000)],\n    num_steps=10000)\nfinal_time_step, final_policy_state = watch_driver.run()\n\nplot_animation(frames)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creando un gif\nimport PIL\nimport os\n\nimage_path = os.path.join(\"view1.gif\")\nframe_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\nframe_images[0].save(image_path, format='GIF',\n                     append_images=frame_images[1:],\n                     save_all=True,\n                     duration=300,\n                     loop=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%html\n<img src=\"view9e5.gif\" />","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n[Hands–On Machine Learning with Scikit–Learn and TensorFlow 2](https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646)\n\n\n[Agents is a library for reinforcement learning in TensorFlow.\n](https://www.tensorflow.org/agents)\n\n\n[Introduction to TF-Agents : A library for Reinforcement Learning in TensorFlow](https://towardsdatascience.com/introduction-to-tf-agents-a-library-for-reinforcement-learning-in-tensorflow-68ab9add6ad6)\n\n\n[Train a Deep Q Network with TF-Agents](https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}