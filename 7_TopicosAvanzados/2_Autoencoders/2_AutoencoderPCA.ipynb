{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO7hy9qWx+QtrGRFIXWEY1v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amalvarezme/AprendizajeMaquina/blob/main/7_TopicosAvanzados/Autoencoders/AutoencoderPCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDVkD7gM1YrH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, losses, Model\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "# Load and prepare the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "scale = 0.4\n",
        "x_train = x_train.astype('float32') / 255. + np.random.normal(scale=scale,size=x_train.shape)\n",
        "x_test = x_test.astype('float32') / 255. + np.random.normal(scale=scale,size=x_test.shape)\n",
        "\n",
        "# create training, validation, and testing sets\n",
        "x_val = x_train[50000:]\n",
        "y_val = y_train[50000:]\n",
        "x_train = x_train[:50000]\n",
        "y_train = y_train[:50000]\n",
        "x_train = x_train[..., tf.newaxis]\n",
        "x_val = x_val[..., tf.newaxis]\n",
        "x_test = x_test[..., tf.newaxis]\n",
        "\n",
        "print(x_train.shape,x_val.shape,x_test.shape,y_train.shape,y_val.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "9czMRgQj5oHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plot original images vs reconstructed images\n",
        "def plot_mnist_autoencoder(x,xpred,cmap='gray',vmin=0,vmax=1):\n",
        "  fig,ax = plt.subplots(2,x.shape[0],figsize=(8,1))\n",
        "  for i,class_ in enumerate(range(x.shape[0])):\n",
        "        ax[0,i].imshow(x[i],cmap=cmap,vmin=vmin,vmax=vmax)\n",
        "        ax[0,i].set_xticks([])\n",
        "        ax[0,i].set_yticks([])\n",
        "\n",
        "        ax[1,i].imshow(xpred[i],cmap=cmap,vmin=vmin,vmax=vmax)\n",
        "        ax[1,i].set_xticks([])\n",
        "        ax[1,i].set_yticks([])\n",
        "  plt.show()\n",
        "  return\n",
        "\n",
        "plot_mnist_autoencoder(x_train[:15],x_train[:15])"
      ],
      "metadata": {
        "id": "C2GUMmcM57Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
        "\n",
        "#plot images on latent space\n",
        "def plot_mnist_2d(Z,y,images,img_w=28,img_h=28,zoom=0.5,cmap='jet'):\n",
        "    fig, ax = plt.subplots(figsize=(5,5))\n",
        "    plt.axis('off')\n",
        "    for i in range(Z.shape[0]):\n",
        "        #print('img',i+1,'/',Z.shape[0])\n",
        "        image = images[i].reshape((img_w, img_h))\n",
        "        im = OffsetImage(image, zoom=zoom,cmap=cmap)\n",
        "        ab = AnnotationBbox(im, (Z[i,0], Z[i,1]), xycoords='data', frameon=False)\n",
        "        ax.add_artist(ab)\n",
        "        ax.update_datalim([(Z[i,0], Z[i,1])])\n",
        "        ax.autoscale()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7elhv1xhDw71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#traditional PCA algorithm\n",
        "red = PCA(n_components=2, random_state=123)\n",
        "Z = red.fit_transform(x_train.reshape(x_train.shape[0],-1))\n",
        "N = 500\n",
        "plot_mnist_2d(Z[:N],y_train[:N],x_train[:N],img_w=28,img_h=28,zoom=0.3,cmap='gray')"
      ],
      "metadata": {
        "id": "cQ470_1MDyoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#custom autoencoder\n",
        "class Autoencoder(Model):\n",
        "    def __init__(self, encoding_dim):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoding_dim = encoding_dim\n",
        "        # Encoder layers\n",
        "        self.encoder_input_layer = layers.Flatten()\n",
        "        self.encoder_dense_layer = layers.Dense(encoding_dim, activation='relu')\n",
        "        # Decoder layers will be initialized in build()\n",
        "        self.decoder_dense_layer = None\n",
        "        self.decoder_output_layer = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Now that we have the input shape, initialize decoder layers\n",
        "        self.decoder_dense_layer = layers.Dense(input_shape[1]*input_shape[2], activation='sigmoid')\n",
        "        self.decoder_output_layer = layers.Reshape(input_shape[1:])\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.encoder_input_layer(inputs)\n",
        "        encoded = self.encoder_dense_layer(x)\n",
        "        x = self.decoder_dense_layer(encoded)\n",
        "        decoded = self.decoder_output_layer(x)\n",
        "        return decoded\n"
      ],
      "metadata": {
        "id": "lceJCEBL8Xdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the autoencoder\n",
        "encoding_dim = 64\n",
        "input_shape = (None, 28, 28, 1)\n",
        "autoencoder = Autoencoder(encoding_dim)\n",
        "autoencoder.build(input_shape)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "kzfCM9fPQNbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.layers[1].get_weights()[0].shape"
      ],
      "metadata": {
        "id": "F5bOlxX5RRzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss object and the optimizer\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Define measures to track loss\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    with tf.GradientTape() as tape:\n",
        "        reconstructed = autoencoder(images, training=True)\n",
        "        loss = loss_object(images, reconstructed)\n",
        "    gradients = tape.gradient(loss, autoencoder.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n",
        "    train_loss(loss)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images):\n",
        "    reconstructed = autoencoder(images, training=False)\n",
        "    t_loss = loss_object(images, reconstructed)\n",
        "    test_loss(t_loss)\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Prepare the dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(buffer_size=1024).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(x_val).shuffle(buffer_size=128).batch(64)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Reset the metrics at the start of each epoch\n",
        "    train_loss.reset_states()\n",
        "    test_loss.reset_states()\n",
        "\n",
        "    for images in train_dataset:\n",
        "        train_step(images)\n",
        "\n",
        "    for val_images in val_dataset:\n",
        "        test_step(val_images)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, '\n",
        "          f'Loss: {train_loss.result()}, '\n",
        "          f'Test Loss: {test_loss.result()}')\n",
        "    if (epoch+1)%5 == 0:\n",
        "      val_reconstructed = autoencoder(val_images, training=False)\n",
        "      print(val_reconstructed.shape)\n",
        "      plot_mnist_autoencoder(val_images,val_reconstructed)\n",
        ""
      ],
      "metadata": {
        "id": "SNyEL0ZyQDAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#latent space of trained autoencoder + PCA2D\n",
        "from sklearn.manifold import TSNE\n",
        "red = TSNE(n_components=2,perplexity=10,random_state=123,verbose=100)\n",
        "N = 500\n",
        "Z = red.fit_transform(autoencoder.layers[1](autoencoder.layers[0](x_val[:N])).numpy().reshape(N,-1))"
      ],
      "metadata": {
        "id": "OPiz3CiaF9mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot 2D tsne from autoencoder latent space\n",
        "plot_mnist_2d(Z[:N],y_val[:N],x_val[:N],img_w=28,img_h=28,zoom=0.3,cmap='gray')\n",
        "plt.scatter(Z[:N,0],Z[:N,1],c=y_val[:N])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LNwhUsWyYkis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute inner product among basis\n",
        "o_ = tf.linalg.matmul(autoencoder.layers[1].get_weights()[0],autoencoder.layers[1].get_weights()[0],transpose_a=True)\n",
        "plt.pcolormesh(o_.numpy())\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QMJaS-V8Hcdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseTransposeLayer(layers.Layer):\n",
        "    def __init__(self, units, factor_o=0.1,activation=None, **kwargs):\n",
        "        super(DenseTransposeLayer, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.factor_o = factor_o\n",
        "        self.activation = tf.keras.activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.w = self.add_weight(\n",
        "            shape=(input_shape, self.units),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,regularizer=tf.keras.regularizers.OrthogonalRegularizer(factor=self.factor_o),\n",
        "            constraint=tf.keras.constraints.max_norm(1.)\n",
        "        )\n",
        "        #self.b1 = self.add_weight(\n",
        "        #    shape=(self.units,), initializer=\"zeros\", trainable=True)\n",
        "        #self.b2 = self.add_weight(\n",
        "        #    shape=(input_shape[-1],), initializer=\"zeros\", trainable=True)\n",
        "\n",
        "        super(DenseTransposeLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = tf.linalg.matmul(inputs, self.w) #+ self.b1\n",
        "        if self.activation is not None:\n",
        "            x = self.activation(x)\n",
        "        x = tf.linalg.matmul(x, tf.transpose(self.w)) #+ self.b2\n",
        "        return x"
      ],
      "metadata": {
        "id": "-9Vj-G66hlV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orthogonal Autoencoder with linear activation : PCA as keras-based NN\n",
        "\n",
        "#custom autoencoder\n",
        "class PCAutoencoder(Model):\n",
        "    def __init__(self, encoding_dim,factor_o=0.1):\n",
        "        super(PCAutoencoder, self).__init__()\n",
        "        self.encoding_dim = encoding_dim\n",
        "        self.factor_o=factor_o\n",
        "        # Encoder layers\n",
        "        self.encoder_input_layer = layers.Flatten()\n",
        "\n",
        "        # Decoder layers will be initialized in build()\n",
        "        self.encoder_decoder_transpose = DenseTransposeLayer(self.encoding_dim, factor_o=self.factor_o,activation='linear')\n",
        "        self.decoder_output_layer = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Now that we have the input shape, initialize decoder layers\n",
        "        self.encoder_decoder_transpose.build(input_shape[1]*input_shape[2])\n",
        "        self.decoder_output_layer = layers.Reshape(input_shape[1:])\n",
        "        super().build(input_shape)\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.encoder_input_layer(inputs)\n",
        "        x = self.encoder_decoder_transpose(x)\n",
        "        decoded = self.decoder_output_layer(x)\n",
        "        return decoded\n"
      ],
      "metadata": {
        "id": "4sATVY2Y4Zn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the autoencoder\n",
        "encoding_dim = 64\n",
        "input_shape = (None, 28, 28, 1)\n",
        "factor_o = 0.1\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)\n",
        "pcautoencoder = PCAutoencoder(encoding_dim,factor_o=factor_o)\n",
        "pcautoencoder.build(input_shape)\n",
        "pcautoencoder.compile(optimizer='adam', loss='mse')\n",
        "pcautoencoder.summary()"
      ],
      "metadata": {
        "id": "uZKYu66i1Tu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pca Autoencoder Training\n",
        "tf.keras.backend.clear_session()\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    with tf.GradientTape() as tape:\n",
        "        reconstructed = pcautoencoder(images, training=True)\n",
        "        loss = loss_object(images, reconstructed)\n",
        "    gradients = tape.gradient(loss, pcautoencoder.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, pcautoencoder.trainable_variables))\n",
        "    train_loss(loss)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images):\n",
        "    reconstructed = pcautoencoder(images, training=False)\n",
        "    t_loss = loss_object(images, reconstructed)\n",
        "    test_loss(t_loss)\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Prepare the dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(buffer_size=1024).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(x_val).shuffle(buffer_size=128).batch(64)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Reset the metrics at the start of each epoch\n",
        "    train_loss.reset_states()\n",
        "    test_loss.reset_states()\n",
        "\n",
        "    for images in train_dataset:\n",
        "        train_step(images)\n",
        "\n",
        "    for val_images in val_dataset:\n",
        "        test_step(val_images)\n",
        "\n",
        "    print(f'Epoch {epoch + 1}, '\n",
        "          f'Loss: {train_loss.result()}, '\n",
        "          f'Test Loss: {test_loss.result()}')\n",
        "    if (epoch+1)%5 == 0:\n",
        "      val_reconstructed = pcautoencoder(val_images, training=False)\n",
        "      print(val_reconstructed.shape)\n",
        "      plot_mnist_autoencoder(val_images,val_reconstructed)\n",
        ""
      ],
      "metadata": {
        "id": "FWeVK-abJK2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute inner product among basis\n",
        "o_ = tf.linalg.matmul(pcautoencoder.layers[1].get_weights()[0],pcautoencoder.layers[1].get_weights()[0],transpose_a=True)\n",
        "plt.pcolormesh(o_.numpy())\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9FyIDCIDdSrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pcolormesh(autoencoder.layers[1].get_weights()[0])\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WchleemYsx-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ow4as9Cpum3x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}